---
title: 'Part 3: Running Models'
author: "Sarthak Chauhan"
date: "2022-12-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(leaps)
library(caret)
library(glmnet)
```


```{r}
#Using forward selection to identify significant parameters
set.seed(1902)
intercept_only <- lm(INFLATION... ~ 1, data=train)
all <- lm(INFLATION... ~ ., data=train)
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova
forward$coefficients
# From the above data we see that the following predictors are the most statistically significant
```

```{r}
#Linear Regression Model
linearModel = lm(INFLATION... ~  PPI.CONST.MAT. + CPIALLITEMS + MORTGAGE.INT..MONTHLY.AVG... + CORP..BOND.YIELD... + MED.HOUSEHOLD.INCOME	+ CSUSHPISA + MONTHLY.HOME.SUPPLY	+ X..SHARE.OF.WORKING.POPULATION, data = train) 
summary(linearModel)
AIC(linearModel)
BIC(linearModel)
```

```{r}
#Logistic Regression Model
logisticRegression = glm(INFLATION... ~  PPI.CONST.MAT. + CPIALLITEMS + MORTGAGE.INT..MONTHLY.AVG... + CORP..BOND.YIELD... + MED.HOUSEHOLD.INCOME	+ CSUSHPISA + MONTHLY.HOME.SUPPLY	+ X..SHARE.OF.WORKING.POPULATION, data = train)
summary(logisticRegression)
BIC(logisticRegression)
```
```{r}
#Ridge Regression Model

set.seed(1902)
#define response variable
y <- train$INFLATION...
#define matrix of predictor variables
x <- data.matrix(train[, c('PPI.CONST.MAT.','CPIALLITEMS', 'MORTGAGE.INT..MONTHLY.AVG...', 'CORP..BOND.YIELD...', 'MED.HOUSEHOLD.INCOME', 'CSUSHPISA', 'MONTHLY.HOME.SUPPLY', 'X..SHARE.OF.WORKING.POPULATION')])
ridgeRegression <- glmnet(x, y, alpha = 0)
summary(ridgeRegression)

#perform k-fold cross-validation to find optimal lambda value
crossModel <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
bestLambda <- crossModel$lambda.min
bestLambda
#produce plot of test MSE by lambda value
plot(crossModel) 
#find coefficients of best model
bestModel <- glmnet(x, y, alpha = 0, lambda = bestLambda)
coef(bestModel)
plot(ridgeRegression, xvar = "lambda")
#use fitted best model to make predictions
yPredicted <- predict(ridgeRegression, s = bestLambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((yPredicted - y)^2)

#find R-Squared
rSquared <- 1 - sse/sst
rSquared
```

```{r}
#Lasso Regression Model
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
#produce plot of test MSE by lambda value
plot(cv_model) 
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
#use fitted best model to make predictions
y_predicted <- predict(best_model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

