---
title: 'Part 3: Running Models'
author: "Sarthak Chauhan"
date: "2022-12-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(leaps)
library(caret)
library(glmnet)
library(randomForest)
library(xgboost)
```


```{r}
#Using forward selection to identify significant parameters
set.seed(1902)
intercept_only <- lm(INFLATION... ~ 1, data=train)
all <- lm(INFLATION... ~ ., data=train)
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova
forward$coefficients
# From the above data we see that the following predictors are the most statistically significant
```

```{r}
#Linear Regression Model
linearModel = lm(INFLATION... ~  PPI.CONST.MAT. + CPIALLITEMS + MORTGAGE.INT..MONTHLY.AVG... + CORP..BOND.YIELD... + MED.HOUSEHOLD.INCOME	+ CSUSHPISA + MONTHLY.HOME.SUPPLY	+ X..SHARE.OF.WORKING.POPULATION, data = train) 
summary(linearModel)
AIC(linearModel)
BIC(linearModel)
```

```{r}
#Logistic Regression Model
logisticRegression = glm(INFLATION... ~  PPI.CONST.MAT. + CPIALLITEMS + MORTGAGE.INT..MONTHLY.AVG... + CORP..BOND.YIELD... + MED.HOUSEHOLD.INCOME	+ CSUSHPISA + MONTHLY.HOME.SUPPLY	+ X..SHARE.OF.WORKING.POPULATION, data = train)
summary(logisticRegression)
BIC(logisticRegression)
```
```{r}
#Ridge Regression Model

set.seed(1902)
#define response variable
y <- train$INFLATION...
#define matrix of predictor variables
x <- data.matrix(train[, c('PPI.CONST.MAT.','CPIALLITEMS', 'MORTGAGE.INT..MONTHLY.AVG...', 'CORP..BOND.YIELD...', 'MED.HOUSEHOLD.INCOME', 'CSUSHPISA', 'MONTHLY.HOME.SUPPLY', 'X..SHARE.OF.WORKING.POPULATION')])
ridgeRegression <- glmnet(x, y, alpha = 0)
summary(ridgeRegression)

#perform k-fold cross-validation to find optimal lambda value
crossModel <- cv.glmnet(x, y, alpha = 0)

#find optimal lambda value that minimizes test MSE
bestLambda <- crossModel$lambda.min
bestLambda
#produce plot of test MSE by lambda value
plot(crossModel) 
#find coefficients of best model
bestModel <- glmnet(x, y, alpha = 0, lambda = bestLambda)
coef(bestModel)
plot(ridgeRegression, xvar = "lambda")
#use fitted best model to make predictions
yPredicted <- predict(ridgeRegression, s = bestLambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((yPredicted - y)^2)

#find R-Squared
rSquaredRidge <- 1 - sse/sst
rSquaredRidge
```

```{r}
#Lasso Regression Model
#perform k-fold cross-validation to find optimal lambda value
crossModel2 <- cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
bestLambda2 <- crossModel2$lambda.min
bestLambda2
#produce plot of test MSE by lambda value
plot(crossModel2) 
#find coefficients of best model
bestModel2 <- glmnet(x, y, alpha = 1, lambda = bestLambda2)
coef(bestModel2)
#use fitted best model to make predictions
yPredicted2 <- predict(bestModel2, s = bestLambda2, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((yPredicted2 - y)^2)

#find R-Squared
rSquaredLasso <- 1 - sse/sst
rSquaredLasso
```

```{r}
#Random Forest Model
randomForestModel <- randomForest(INFLATION... ~  ., data = train, mtry = 3, importance = TRUE, na.action = na.omit)
randomForestModel
plot(randomForestModel)
summary(randomForestModel)

which.min(randomForestModel$mse)
sqrt(randomForestModel$mse[which.min(randomForestModel$mse)])
```
```{r}
#XGBOOST MODEL
train_x = data.matrix(train[, -5])
train_y = train[,5]
test_x = data.matrix(test[, -5])
test_y = test[, 5]
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
watchlist = list(train=xgb_train, test=xgb_test)
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 70) #From the output we can see that the minimum testing RMSE is achieved at 26 rounds.
final = xgboost(data = xgb_train, max.depth = 3, nrounds = 26, verbose = 0) #weâ€™ll define our final XGBoost model to use 26 rounds
pred_y = predict(final, test_x)
mean((test_y - pred_y)^2) #mse
caret::MAE(test_y, pred_y) #mae
caret::RMSE(test_y, pred_y) #rmse
```

